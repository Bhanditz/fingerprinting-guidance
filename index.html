<!DOCTYPE html>
<html>
  <head>
    <title>Mitigating Browser Fingerprinting in Web Specifications</title>
    <meta charset='utf-8'>
    <script src='https://www.w3.org/Tools/respec/respec-w3c-common' class='remove'></script>
    <script class='remove'>
      var respecConfig = {
          // specification status (e.g. WD, LCWD, NOTE, etc.). If in doubt use ED.
          //specStatus:           "IG-NOTE",
          specStatus: "ED",
          noRecTrack: true,
          //publishDate: "2016-07-06",
          // the specification's short name, as in http://www.w3.org/TR/short-name/
          shortName:            "fingerprinting-guidance",

          // if your specification has a subtitle that goes below the main
          // formal title, define it here
          // subtitle   :  "an excellent document",

          // if you wish the publication date to be other than today, set this
          // publishDate:  "2009-08-06",

          // if the specification's copyright date is a range of years, specify
          // the start date here:
          // copyrightStart: "2005"

          // if there is a previously published draft, uncomment this and set its YYYY-MM-DD date
          // and its maturity status
          previousPublishDate:  "2015-11-25",
          previousMaturity:  "IG-NOTE",

          // if there a publicly available Editor's Draft, this is the link
          edDraftURI:           "http://w3c.github.io/fingerprinting-guidance/",

          // if this is a LCWD, uncomment and set the end of its review period
          // lcEnd: "2009-08-05",

          // editors, add as many as you like
          // only "name" is required
          editors:  [
              { name: "Nick Doty", url: "https://npdoty.name/" },
          ],
          otherLinks: [{
              key: "Version history",
              data: [{
                value: "GitHub commit history",
                href: "https://github.com/w3c/fingerprinting-guidance/commits/gh-pages"
           }]},
           {
                         key: "Issues list",
                         data: [{
                           value: "GitHub issues list",
                           href: "https://github.com/w3c/fingerprinting-guidance/issues"
                      }]
           }
           ],          

          // name of the WG
          wg:           "Privacy Interest Group",
          
          // URI of the public WG page
          wgURI:        "http://www.w3.org/Privacy/",
          
          // name (without the @w3c.org) of the public mailing to which comments are due
          wgPublicList: "public-privacy",
          
          // URI of the patent status for this WG, for Rec-track documents
          // !!!! IMPORTANT !!!!
          // This is important for Rec-track documents, do not copy a patent URI from a random
          // document unless you know what you're doing. If in doubt ask your friendly neighbourhood
          // Team Contact.
          wgPatentURI:  "",
          charterDisclosureURI: "http://www.w3.org/2011/07/privacy-ig-charter.html",
          
          localBiblio: {
            "EVERCOOKIE": {
              "authors": ["Samy Kamkar"],
              "href": "http://samy.pl/evercookie/",
              "title": "evercookie - virtually irrevocable persistent cookies",
              "date": "September 2010"
            },
            "NDSS-FINGERPRINTING": {
              "authors": ["Ting-Fang Yen", "Yinglian Xie", "Fang Yu", "Roger Peng Yu", "Martin Abadi"],
              "href": "http://research.microsoft.com/apps/pubs/default.aspx?id=156901",
              "title": "Host Fingerprinting and Tracking on the Web: Privacy and Security Implications",
              "date": "February 2012",
              "publisher": "In Proceedings of the Network and Distributed System Security Symposium (NDSS)"
            },
            "RFC6973": {
              "authors": [
                  "A. Cooper",
                  "H. Tschofenig",
                  "B. Aboba",
                  "J. Peterson",
                  "J. Morris",
                  "M. Hansen",
                  "R. Smith"
              ],
              "href": "http://www.rfc-editor.org/rfc/rfc6973.txt",
              "title": "Privacy Considerations for Internet Protocols",
              "date": "July 2013",
              "status": "RFC",
              "publisher": "IETF"
            },
            "WEBSTORAGE-user-tracking": {
              "href": "http://www.w3.org/TR/2013/REC-webstorage-20130730/#user-tracking",
              "title": "Web Storage > Privacy > User tracking",
              "date": "July 2013",
              "authors": [
                "Ian Hickson"
              ],
              "status": "Rec",
              "publisher": "W3C"
            },
            "TRACKING-DNT": {
              "href": "http://www.w3.org/TR/tracking-dnt/",
              "title": "Tracking Preference Expression (DNT)",
              "date": "April 2014",
              "authors": [
                "Roy Fielding", "David Singer"
              ],
              "status": "LCWD",
              "publisher": "W3C"
            },
            "TRACKING-COMPLIANCE": {
              "href": "http://www.w3.org/TR/tracking-compliance/",
              "title": "Tracking Compliance and Scope",
              "date": "July 2015",
              "authors": [
                "Nick Doty", "Justin Brookman", "Heather West", "Sean Harvey", "Erica Newland"
              ],
              "status": "LCWD",
              "publisher": "W3C"
            },
            "TAG-UNSANCTIONED": {
              "href": "https://w3ctag.github.io/unsanctioned-tracking/",
              "title": "Unsanctioned Web Tracking",
              "date": "17 July 2015",
              "authors": ["Mark Nottingham"],
              "publisher": "W3C Technical Architecture Group"
            }
          }
      };
      </script>
      <style type="text/css" media="screen">
        img.fingerprint {
          float: left; 
          margin-left: -25px;
        }
        ul.practicedesc {
            margin-top: -2em;
            padding-top: .5em;
        }
      </style>
  </head>
  <body>
    <section id="abstract">
      Exposure of settings and characteristics of browsers can harm user privacy by allowing for browser fingerprinting. This document defines different types of fingerprinting, considers distinct levels of mitigation for the related privacy risks and provides guidance for Web specification authors on how to balance these concerns when designing new Web features.
    </section>
    <section id="sotd">
      This document is a draft Interest Group Note to provide guidance to Web specification authors on mitigating the privacy impacts of browser fingerprinting, currently under development by the <a href="http://www.w3.org/Privacy/">Privacy Interest Group</a> (<abbr title="Privacy Interest Group">PING</abbr>). <a href="https://www.w3.org/TR/2015/NOTE-fingerprinting-guidance-20151124/">A snapshot draft of this Note was published on 24 November 2015</a>. PING is collaborating with the <a href="http://www.w3.org/2001/tag/">Technical Architecture Group</a> (<abbr title="Technical Architecture Group">TAG</abbr>) on this guidance. Constructive input of all kinds would be useful; feel free to contact the editor directly, send comments to the <a href="mailto:public-privacy@w3.org">mailing list</a> or <a href="https://github.com/w3c/fingerprinting-guidance/issues">file issues on GitHub</a>.
    </section>
    <section>
      <h2>Browser fingerprinting</h2>
        <section>
        <h2>What is fingerprinting?</h2>
        <p>In short, <dfn>browser fingerprinting</dfn> is the capability of a site to identify or re-identify a visiting user, user agent or device via configuration settings or other observable characteristics.</p>
        <p>A similar definition is provided by [[RFC6973]]. A more detailed list of types of fingerprinting is included below. This document does not attempt to catalog all features currently used or usable for browser fingerprinting; however, <a href="#research"></a> provides links to browser vendor pages and academic findings.</p>
        </section>
        <section>
        <h2 id="privacy_threat_models">Privacy impacts and threat models</h2>

        <p>Browser fingerprinting can be used as a security measure (e.g. as means of authenticating the user). However, fingerprinting is also a potential threat to users' privacy on the Web. This document does not attempt to provide a single unifying definition of "privacy" or "personal data", but we highlight how browser fingerprinting might impact users' privacy. For example, browser fingerprinting can be used to:</p>
        <ul>
          <li>identify a user</li>
          <li>correlate a user’s browsing activity within and across sessions</li>
          <li>track users without transparency or control</li>
        </ul>

        <p>The privacy implications associated with each use case are discussed below. Following from the practice of security threat model analysis, we note that there are distinct models of privacy threats for fingerprinting. Defenses against these threats differ, depending on the particular privacy implication and the threat model of the user.</p>

        <section>
          <h3>Identify a user</h3>
          <p>There are many reasons why users might wish to remain anonymous or unidentified online, including: concerns about surveillance, personal physical safety, concerns about discrimination against them based on what they read or write when using the Web. When a browser fingerprint is correlated with identifying information (like an email address or legal name), an application or service provider may be able to identify an otherwise pseudonymous user.</p>
          <p>Users concerned about physical safety from, for example, a governmental adversary might employ onion routing systems such as Tor to limit network-level linkability but still face the danger of browser fingerprinting to correlate their Web-based activity.</p>
          <p class="note">Is this privacy implication usefully distinct from unexpected correlation? How does this relate to linkability to other identities? [via TAG feedback]</p>
        </section>
        
        <section>
          <h3>Correlation of browsing activity</h3>
          <p>Browser fingerprinting raises privacy concerns even when real-world identities are not implicated. Some users may be surprised or concerned that an online party can correlate multiple visits (on the same or different sites) to develop a profile or history of the user. This concern may be heightened because (see below) it may occur without the user's knowledge or consent and tools such as clearing cookies do not prevent further correlation.</p>
          <p>Browser fingerprinting also allows for tracking across <a class="externalDFN" href="http://tools.ietf.org/html/rfc6454#section-4">origins</a> [[RFC6454]]: different sites may be able to combine information about a single user even where a cookie policy would block accessing of cookies between origins, because the fingerprint is relatively unique and the same for all origins.</p>
        </section>
        
        <section>
          <h3>Tracking without transparency or user control</h3>
          <p>
            In contrast to other mechanisms defined by Web standards for maintaining state (e.g. cookies), browser fingerprinting allows for collection of data about user activity without clear indications that such collection is happening. Transparency can be important for end users, to understand how ongoing collection is happening, but it also enables researchers, policymakers and others to document or regulate privacy-sensitive activity. Browser fingerprinting also allows for tracking of activity without clear or effective user controls: a browser fingerprint typically cannot be cleared or re-set. (See the finding on unsanctioned tracking [[TAG-UNSANCTIONED]].)
          </p>
        </section>
    </section>
    <section>
      <h2>What can we do about it?</h2>
      <p>
        Advances in techniques for browser fingerprinting (see <a href="#research"></a>, below), particularly in <a>active fingerprinting</a>, suggest that elimination of the capability of browser fingerprinting by a determined adversary through solely technical means that are widely deployed is implausible. However, mitigations in our technical specifications are possible, as described below (<a href="#mitigations"></a>), and may achieve different levels of success (<a href="#feasibility"></a>).
      </p>
      <p>
        Mitigations recommended here are simply mitigations, not solutions. Users of the Web cannot confidently rely on sites being completely unable to correlate traffic, especially when executing client-side code. A fingerprinting surface extends across all implemented Web features for a particular user agent, and even to other layers of the stack; for example, differences in TCP connections. In order to mitigate the risk as a whole, fingerprinting must be considered during the design and development of all specifications.</p>
      <p>The TAG finding on Unsanctioned Web Tracking, including browser fingerprinting, includes description of the limitations of technical measures and encourages minimizing and documenting new fingerprinting surface [[TAG-UNSANCTIONED]]. The best practices below detail common actions that authors of specifications for Web features can take to mitigate the privacy impacts of browser fingerprinting.
      </p>
    </section>
    </section>
    
    <section id="bp-summary"></section>
    
    <section>
        <h2 id="types_of_fingerprinting">Types of fingerprinting</h2>
        <section>
          <h3 id="passive">Passive</h3>
          <p><dfn>Passive fingerprinting</dfn> is browser fingerprinting based on characteristics observable in the contents of Web requests, without the use of any code executed on the client.</p>
          <p>Passive fingerprinting would trivially include cookies (often unique identifiers sent in HTTP requests), the set of HTTP request headers and the IP address and other network-level information. The <a href="http://tools.ietf.org/html/rfc2616#section-14.43">User-Agent string</a>, for example, is an HTTP request header that typically identifies the browser, renderer, version and operating system. For some populations, the User-Agent string and IP address will commonly uniquely identify a particular user's browser [[NDSS-FINGERPRINTING]].</p>
        </section>
        <section>
          <h3 id="active">Active</h3>
          <p>For <dfn>active fingerprinting</dfn>, we also consider techniques where a site runs JavaScript or other code on the local client to observe additional characteristics about the browser. Techniques for active fingerprinting might include accessing the window size, enumerating fonts or plug-ins, evaluating performance characteristics, or rendering graphical patterns. Key to this distinction is that <a>active fingerprinting</a> takes place in a way that is potentially detectable on the client.</p>
        </section>
        <section>
          <h3 id="cookie_like_setting_retrieving_local_state">Cookie-like (setting/retrieving local state)</h3>
          <p>Users, user agents and devices may also be re-identified by a site that first sets and later retrieves state stored by a user agent or device. This <dfn>cookie-like fingerprinting</dfn> allows re-identification of a user or inferences about a user in the same way that HTTP cookies allow state management for the stateless HTTP protocol [[RFC6265]].</p>
          <p>Cookie-like fingerprinting can also circumvent user attempts to limit or clear cookies stored by the user agent, as demonstrated by the "evercookie" implementation [[EVERCOOKIE]]. Where state is maintained across user agents (as in the case of common plugins with local storage), across devices (as in the case of certain browser syncing mechanisms) or across software upgrades, cookie-like fingerprinting can allow re-identification of users, user agents or devices where active and passive fingerprinting might not.</p>
        </section>
    </section>

    <section>
      <h2 id="feasibility">Feasibility</h2>
      
      <section>
          <h2>Fingerprinting mitigation levels of success</h2>
          <p>There are different levels of success in addressing browser fingerprinting:</p>
          <dl>
          <dt>Decreased fingerprinting surface</dt><dd>Removing the source of entropy or accessible attributes that can be used for fingerprinting.</dd>
          <dt>Increased anonymity set</dt><dd>By standardization, convention or common implementation, increasing the commonality of particular configurations to decrease the likelihood of unique fingerprintability.</dd>
          <dt>Detectable fingerprinting</dt><dd>Making (in particular, client-side) fingerprinting observable to others, so that the user agent might block it or researchers can determine that it's happening.</dd>
          </dl>
      </section>
      <section>
        <h2>Feasible goals for specification authors</h2>
        <p>
          This document works under the expectation that mitigations with different levels of success are feasible under different circumstances, for different threat models and against different types of fingerprinting. In general, active fingerprinting may be made detectable; we can minimize increases to the surface of passive fingerprinting; and cookie-like fingerprinting can be documented to enable clearing local state.</p>
        <p>
          Some implementers and some users may be willing to accept reduced functionality or decreased performance in order to minimize browser fingerprinting. Documenting which features have fingerprinting risk eases the work of implementers building modes for these at-risk users; minimizing fingerprinting even in cases where common implementations will have easy active fingerprintability allows such users to reduce the functionality trade-offs necessary. Making browser fingerprinting more detectable also contributes to mitigations outside the standardization process; for example, though regulatory or policy means  [[TAG-UNSANCTIONED]].
        </p>
      </section>
    </section>
    <section>
        <h2 id="mitigations">Mitigations</h2>
        <section>
          <h3 id="weighing_increased_fingerprinting_surface">Weighing increased fingerprinting surface</h3>
          <p>The <dfn>fingerprinting surface</dfn> of a user agent is the set of observable characteristics that can be used in concert to identify a user, user agent or device or correlate its activity. Web specification authors regularly attempt to strike a balance between new functionality and fingerprinting surface. For example, feature detection functionality allows for progressive enhancement with a small addition to fingerprinting surface; detailed enumerations of plugins, fonts, connected devices may provide a large fingerprinting surface with minimal functional support.</p>
          <p>Authors and Working Groups determine the appropriate balance between these properties on a case-by-case basis, given their understanding of the functionality, its likely implementations and the entropy of increased fingerprinting surface. However, given the distinct privacy impacts described above and in order to improve consistency across specifications, these practices provide some guidance:</p>
          
          <div class="practice">
            <p>
              <span id="avoid-passive-increases" class="practicelab">Avoid unnecessary increases to the surface for passive fingerprinting.</span>
            </p>
            <p class="practicedesc">
              Unless a feature cannot reasonably be designed in any other way, increased passive fingerprintability should be avoided. Passive fingerprinting surface allows for much easier fingerprinting, without opportunities for external detection (by users or third parties).
            </p>
          </div>
          <div class="practice">
            <p>
              <span id="avoid-active-increases" class="practicelab">Prefer functionally-comparable designs that don't increase the surface for active fingerprinting.</span>
            </p>
            <p class="practicedesc">
              If comparable functionality could be accomplished without increasing the surface for active fingerprinting, prefer the less fingerprintable alternative. Defining "equivalent" or "comparable" functionality can be difficult; use your best judgment and avoid unnecessary fingerprintability.       
            </p>
          </div>
          
          <p>The difference between these practices recognizes that passive fingerprinting surface has lesser options for mitigation (lacking external detectability and client-side preventability) and greater feasibility for reduction.</p>
          
          <div class="practice">
            <p>
              <span id="mark-fingerprinting" class="practicelab">Mark features that contribute to fingerprintability.</span>
            </p>
            <p class="practicedesc">
              <img src="http://www.w3.org/TR/html5/images/fingerprint.png" class="fingerprint" alt="This feature may contribute to browser fingerprintability.">
              Where a feature does contribute to the fingerprinting surface, indicate that impact, by explaining the effect (and any known implementer mitigations) and marking the relevant section with a fingerprinting icon, as this paragraph is.            
            </p>
            <p class="note">
            This practice (and this image) is drawn from the HTML5 specification, which uses it throughout. Can we get feedback from the HTML WG or from readers of that specification as to whether the practice has been useful?
            </p>
          </div>
        </section>
        <section>
			<h3 id="a_standardized_profile">Standardization</h3>
			<p>
        Specifications can mitigate against fingerprintability through standardization; by defining a consistent behavior, conformant implementations won't have variations that can be used for browser fingerprinting.
			</p>
      <p>
        Randomization of certain browser characteristics has been proposed as a way to combat browser fingerprinting. While this strategy may be pursued by some implementations, we expect in general it will be more effective for us to standardize or null values rather than setting a range over which they can vary. The <a href="https://www.torproject.org/projects/torbrowser/design/#idp55149888">Tor Browser design</a> provides more detailed information, but in short: it's difficult to measure how well randomization will work as a mitigation and it can be costly to implement in terms of usability (varying functionality or design in unwanted ways), processing (generating random numbers) and development (including the cost of introducing new security vulnerabilities).
      </p>
      <div class="practice">
        <p>
          <span id="specify-ordering" class="practicelab">Specify orderings and non-functional differences.</span>
        </p>
        <p class="practicedesc">
          To reduce unnecessary entropy, specify aspects of API return values and behavior that don't contribute to functional differences. For example, if the ordering of return values in a list has no semantic value, specify a particular ordering (alphabetical order by a defined algorithm, for example) so that incidental differences don't expose fingerprinting surface.
        </p>
        <p class="practicedesc">
          Access to a list of system fonts via Flash or Java plugins notably returns the list sorted not in a standard alphabetical order, but in an unspecified order specific to the system. This ordering adds to the entropy available from that plugin in a way that provides no functional advantage. (See <a href="https://trac.webkit.org/wiki/Fingerprinting#ii.CollectingSystemFontsviaFlashPlugins">Collecting System Fonts via Flash Plugins</a>.)
        </p>
      </div>
      <p>
        Standardization does <em>not</em> need to attempt to hide all differences between different browsers (e.g. Edge and Chrome); implemented functionality and behavior differences will always exist between different implementations. For that reason, removing <code>User-Agent</code> headers altogether is not a goal. However, variation in the <code>User-Agent</code> string that reveals additional information about the user or device has been shown to provide substantial fingerprinting surface (see "Beauty and the Beast" in <a href="#research"></a>, below).
      </p>
		</section>
        <section>
          <h3>Detectability</h3>
          <p>Where a client-side API provides some fingerprinting surface, authors can still mitigate the privacy concerns via detectability. If client-side fingerprinting activity is to some extent distinguishable from functional use of APIs, user agent implementations may have an opportunity to prevent ongoing fingerprinting or make it observable to users and external researchers (including academics or relevant regulators) who may be able to detect and investigate the use of fingerprinting.</p>
          
          <div class="practice">
            <p>
              <span id="api-minimization" class="practicelab">Design APIs to access only the entropy necessary.</span>
            </p>
            <p class="practicedesc">
              Following the basic principle of <a class="externalDFN" href="https://tools.ietf.org/html/rfc6973#section-6.1">data minimization</a> [[RFC6973]], design your APIs such that a site can access (and does access by default) only the entropy necessary for particular functionality.
            </p>
            <p class="practicedesc">
              Authors might design an API to allow for querying of a particular value, rather than returning an enumeration of all values. User agents and researchers can then more easily distinguish between sites that query for one or two particular values (gaining minimal entropy) and those that query for all values (more likely attempting to fingerprint the browser); or implementations can cap the number of different values. For example, Tor Browser limits the number of fonts that can be queried with a <code>browser.display.max_font_attempts</code> preference.
            </p>
            <p class="practicedesc">
              The granularity or precision of information returned can be minimized in order to reduce entropy. For example, implementations of the Battery Status API [[BATTERY-STATUS]] allowed for high precision (double-precision, or 15-17 significant digits) readings of the current battery level, which provided a short-term identifier that could be used to correlate traffic across origins or clearance of local state. Rounding off values to lower precision mitigates browser fingerprinting while maintaining functional use cases. Alternatively, providing Boolean or a small enumeration of values might provide functionality without revealing underlying details; for example, the Boolean <code>near</code> property in the Proximity Sensor API [[PROXIMITY]].
            </p>
            <p class="practicedesc">
              For more information, see:
            </p>
            <ul class="practicedesc">
              <li><a href="http://www.w3.org/2001/tag/doc/APIMinimization">Data Minimization in Web APIs</a>, Draft TAG Finding, September 2011</li>
              <li><a href="http://www.w3.org/TR/dap-privacy-reqs/">Device API Privacy Requirements</a>, <abbr title="Device APIs Working Group">DAP</abbr> Working Group Note, June 2010</li>
              <li><a href="https://w3c.github.io/sensors/#security-and-privacy">Generic Sensor API: Security and privacy considerations</a>, Editor's Draft, May 2016.</li>
              <li><a href="http://eprint.iacr.org/2015/616.pdf">The leaking battery: A privacy analysis of the HTML5 Battery Status API</a>, February 2016.</li>
            </ul>
          </div>     
          
          <p>
            If your specification exposes some fingerprinting surface (whether it's active or passive), some implementers (e.g. the <a href="https://www.torproject.org/projects/torbrowser.html.en">Tor Browser</a>) are going to be compelled to disable those features for certain privacy-conscious users.
          </p>
          <div class="practice">
            <p>
              <span id="anticipate-disabled" class="practicelab">Enable graceful degradation for privacy-conscious users or implementers.</span>
            </p>
            <p class="practicedesc">
              Following the principle of progressive enhancement, and to avoid further divergence (which might itself expose variation in users), consider whether some functionality in your specification is still possible if fingerprinting surface features are disabled. 
            </p>
            <p class="practicedesc">
              Explicit hooks or API flags may be used so that browser extensions or certain user agents can easily disable specific features. For example, the <a href="http://www.w3.org/TR/html5/scripting-1.html#security-with-canvas-elements">origin-clean flag</a> allows control over whether an image canvas can be read, a significant fingerprinting surface.
            </p>
          </div>     
        </section>
        <section>
          <h3>Clearing all local state</h3>
          <p>Features which enable storage of data on the client and functionality for client- or server-side querying of that data can increase the ease of cookie-like fingerprinting. Storage can vary between large amounts of data (for example, the Web Storage API) or just a binary flag (has or has not provided a certain permission; has or has not cached a single resource).</p>
          <div class="practice">
            <p>
              <span id="no-new-cookies" class="practicelab">Avoid unnecessary new local state mechanisms.</span>
            </p>
            <p class="practicedesc">
              If functionality does not require maintaining client-side state in a way that is subsequently queryable (or otherwise observable), avoid creating a new cookie-like feature. Can the functionality be accomplished with existing HTTP cookies or an existing JavaScript local storage API?
            </p>
            <p class="practicedesc">
              For example, the Flash plugin's Local Shared Objects (LSOs) have often been used to duplicate and re-spawn HTTP cookies cleared by the user. (See "<a href="http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1446862">Flash Cookies and Privacy</a>", Soltani et al., August 2009.)
            </p>
          </div>
          <p>Where features do require setting and retrieving local state, there are ways to mitigate the privacy impacts related to unexpected cookie-like behavior; in particular, you can help implementers prevent "permanent", "zombie", "super" or "evercookies".</p>
          <div class="practice">
            <p>
              <span id="mark-cookie-like" class="practicelab">Highlight any local state mechanisms to enable simultaneous clearing.</span>
            </p>
            <p class="practicedesc">
              Clearly note where state is being maintained and could be queried and provide guidance to implementers on enabling simultaneous deletion of local state for users. Such functionality can mitigate the threat of "evercookies"  because the presence of state in one such storage mechanism can't be used to persist and re-create an identifier. As a result, your design should not rely on saving and later querying data on the client beyond a user's clearing all local state. That is, you should not expect any local state information to be permanent or to persist longer than other local state.
            </p>
          </div>
          <p>Though not strictly browser fingerprinting, there are other privacy concerns regarding user tracking for features that provide local storage of data. Mitigations suggested in the Web Storage API specification include: white-listing, black-listing, expiration and secure deletion [[WEBSTORAGE-user-tracking]].</p>
        </section>
        <section><h3 id="do_not_track_a_cooperative_approach">Do Not Track</h3>
          <p>Expressions of, and compliance with, a Do Not Track signal does not inhibit the capability of browser fingerprinting, but may mitigate some user concerns about fingerprinting, specifically around tracking as defined in those specifications [[TRACKING-DNT]] [[TRACKING-COMPLIANCE]] and as implemented by services that comply with those user preferences. That is, DNT can mitigate concerns with cooperative sites.</p>
          <p>The use of <abbr title="Do Not Track">DNT</abbr> in this way typically does not require changes to other functional specifications. 
            If your specification expects a particular behavior upon receiving a particular DNT signal, indicate that with a reference to [[TRACKING-DNT]].
            If your specification introduces a new communication channel that could be used for tracking, you might wish to define how a DNT signal should be communicated.
          </p>
        </section>
    </section>
    
    <section class="appendix">
        <h2 id="research">Research</h2>
        <p><em>Some browser developers maintain pages on browser fingerprinting, including: potential mitigations or modifications necessary to decrease the surface of that browser engine; different vectors that can be used for fingerprinting; potential future work. These are not cheery, optimistic documents.</em></p>
        <ul>
          <li>"<a href="https://sites.google.com/a/chromium.org/dev/Home/chromium-security/client-identification-mechanisms#TOC-Fingerprinting-prevention-and-detection-challenges">Technical analysis of client identification mechanisms</a>". The Chromium Projects.</li>
          <li><a href="https://trac.webkit.org/wiki/Fingerprinting">WebKit Wiki: Fingerprinting</a></li>
          <li><a href="https://wiki.mozilla.org/Fingerprinting">Mozilla Wiki: Fingerprinting</a></li>
          <li><a href="https://www.torproject.org/projects/torbrowser/design/#fingerprinting-linkability">The Design and Implementation of the Tor Browser: Cross-Origin Fingerprinting Unlinkability</a></li>
        </ul>
        
        <p><em>What are the key papers to read here, historically or to give the latest on fingerprinting techniques? What are some areas of open research that might be relevant?</em></p>
        <ul>
          <li>Eckersley, Peter. "<a href="https://panopticlick.eff.org/browser-uniqueness.pdf">How unique is your web browser?</a>" <i>Privacy Enhancing Technologies</i>. Springer Berlin Heidelberg, 2010.</li>
          <li>Mowery, Keaton, Dillon Bogenreif, Scott Yilek, and Hovav Shacham. “<a href="http://w2spconf.com/2011/papers/jspriv.pdf">Fingerprinting Information in JavaScript Implementations</a>.” In <i>Web 2.0 Security and Privacy</i>, 2011.</li>
          <li>Yen, Ting-Fang, et al. "<a href="http://www.internetsociety.org/host-fingerprinting-and-tracking-web-privacy-and-security-implications">Host fingerprinting and tracking on the web: Privacy and security implications</a>." <em>Proceedings of NDSS</em>. 2012.</li>
          <li>Mowery, Keaton, and Hovav Shacham. "Pixel perfect: Fingerprinting canvas in HTML5." <i>Web 2.0 Security and Privacy</i>, 2012.</li>
          <li id="wsj-orbitz">Mattioli, Dana. "<a href="http://www.wsj.com/articles/SB10001424052702304458604577488822667325882">On Orbitz, Mac Users Steered to Pricier Hotels</a>". <i>Wall Street Journal</i>, August 23, 2012.</li>
          <li id="FPDetective">Gunes Acar et al. "<a href="http://dl.acm.org/citation.cfm?id=2516674">FPDetective: dusting the web for fingerprinters</a>." In <i>CCS '13</i>.</li>
          <li>Nikiforakis, Nick, et al. "<a href="https://seclab.cs.ucsb.edu/media/uploads/papers/sp2013_cookieless.pdf">Cookieless monster: Exploring the ecosystem of web-based device fingerprinting</a>." <i>IEEE Symposium on Security and Privacy (S&amp;P 2013)</i>, 2013.</li>
          <li>G. Acar, C. Eubank, S. Englehardt, M. Juarez, A. Narayanan, C. Diaz. "<a href="https://securehomes.esat.kuleuven.be/%7Egacar/persistent/">The Web never forgets: Persistent tracking mechanisms in the wild</a>." In <i>Proceedings of CCS 2014</i>, Nov. 2014.</li>
          <li>Steven Englehardt, Arvind Narayanan. "<a href="https://webtransparency.cs.princeton.edu/webcensus/">Online tracking: A 1-million-site measurement and analysis</a>." May 2016.</li>
          <li>Pierre Laperdrix, Walter Rudametkin, Benoit Baudry. "<a href="https://hal.inria.fr/hal-01285470v2/">Beauty and the Beast: Diverting modern web browsers to build unique browser fingerprints</a>." <i>IEEE Symposium on Security and Privacy (S&amp;P 2016)</i>, May 2016.</li>
        </ul>
        
        <h3>Testing</h3>
        <p><em>A non-exhaustive list of sites that allow the visitor to test their configuration for fingerprintability.</em></p>
        <ul>
          <li><a href="https://amiunique.org/">amiunique.org</a> (INRIA)</li>
          <li><a href="https://panopticlick.eff.org/">panopticlick.eff.org</a> (EFF)</li>
          <li><a href="http://browserspy.dk/">BrowserSPY.dk</a></li>
          <li><a href="http://fingerprint.pet-portal.eu/">pet-portal cross-browser fingerprinting test</a></li>
          <li><a href="http://lcamtuf.coredump.cx/p0f3/">p0f v3</a> (purely passive fingerprinting)</li>
        </ul>
    </section>    
    
    <section class="appendix">
      <h2>Acknowledgements</h2>
      <p>
        Many thanks to Robin Berjon for ReSpec and to Tobie Langel for Github advice; to the Privacy Interest Group and the Technical Architecture Group for review; to the Tor Browser designers for references and recommendations; and to Christine Runnegar for contributions.
      </p>
    </section>
  </body>
</html>
